{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" \n",
    "     width=\"30%\" \n",
    "     align=right\n",
    "     alt=\"Dask logo\">\n",
    "\n",
    "# Embarrassingly parallel Workloads\n",
    "\n",
    "This notebook shows how to use Dask to parallelize embarrassingly parallel workloads where you want to apply one function to many pieces of data independently.  It will show three different ways of doing this with Dask:\n",
    "\n",
    "1. [dask.delayed](http://dask.pydata.org/en/latest/delayed.html) \n",
    "2. [concurrent.Futures](https://dask.pydata.org/en/latest/futures.html) \n",
    "3. [dask.bag](https://dask.pydata.org/en/latest/bag.html)\n",
    "\n",
    "This example focuses on using Dask for building large embarrassingly parallel computation as often seen in scientific communities and on High Performance Computing facilities, for example with Monte Carlo methods. This kind of simulation assume the following:\n",
    "\n",
    " - We have a function that runs a heavy computation given some parameters.\n",
    " - We need to compute this function on many different input parameters, each function call being independent.\n",
    " - We want to gather all the results in one place for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dask Client for Dashboard\n",
    "\n",
    "Starting the Dask Client will provide a dashboard which \n",
    "is useful to gain insight on the computation.  We will also need it for the\n",
    "Futures API part of this example. Moreover, as this kind of computation\n",
    "is often launched on super computer or in the Cloud, you will probably end\n",
    "up having to start a cluster and connect a client to scale.  See \n",
    "[dask-jobqueue](https://github.com/dask/dask-jobqueue),\n",
    "[dask-kubernetes](https://github.com/dask/dask-kubernetes) or \n",
    "[dask-yarn](https://github.com/dask/dask-yarn) for easy ways to achieve this\n",
    "on respectively an HPC, Cloud or Big Data infrastructure.\n",
    "\n",
    "The link to the dashboard will become visible when you create the client below.  We recommend having it open on one side of your screen while using your notebook on the other side.  This can take some effort to arrange your windows, but seeing them both at the same time is very useful when learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 12:09:55,194 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff3a2d8f3e0>>, <Task finished name='Task-16' coro=<SpecCluster._correct_state_internal() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:346> exception=FileNotFoundError(2, 'No such file or directory')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "          ^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/tornado/ioloop.py\", line 774, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 390, in _correct_state_internal\n",
      "    await asyncio.gather(*worker_futs)\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/tasks.py\", line 684, in _wrap_awaitable\n",
      "    return await awaitable\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-34' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-38' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-37' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-33' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-31' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-32' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-36' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-39' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-30' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-35' coro=<_wrap_awaitable() done, defined at /sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py:124> exception=FileNotFoundError(2, 'No such file or directory')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 125, in _wrap_awaitable\n",
      "    return await aw\n",
      "           ^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 74, in _\n",
      "    await self.start()\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 426, in start\n",
      "    out = await self._submit_job(fn)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 409, in _submit_job\n",
      "    return await self._call(shlex.split(self.submit_command) + [script_filename])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 494, in _call\n",
      "    proc = await asyncio.create_subprocess_exec(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_events.py\", line 1744, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 211, in _make_subprocess_transport\n",
      "    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/base_subprocess.py\", line 36, in __init__\n",
      "    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/asyncio/unix_events.py\", line 820, in _start\n",
      "    self._proc = subprocess.Popen(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/sdf/home/s/sanjeev/miniforge3/envs/dask/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sbatch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -A facet\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 00:10:00\n",
      "#SBATCH -q debug\n",
      "#SBATCH --partition=milano\n",
      "#SBATCH --qos=preemptable\n",
      "source ~/.bashrc\n",
      "/sdf/home/s/sanjeev/miniforge3/envs/dask/bin/python -m distributed.cli.dask_worker tcp://172.24.48.122:37895 --name dummy-name --nthreads 1 --memory-limit 9.31GiB --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "# Simplify SLURMCluster parameters to basic working configuration\n",
    "cluster_kwargs = {\n",
    "    \"cores\": 1,  \n",
    "    \"memory\": \"10GB\", \n",
    "    \"shebang\": \"#!/bin/bash\",\n",
    "    \"account\": \"facet\",\n",
    "    \"walltime\": \"00:10:00\",\n",
    "    \"job_script_prologue\": [\"source ~/.bashrc\"],\n",
    "    # Only basic directives, add more as needed and test each step\n",
    "    \"job_extra_directives\": [\"-q debug\", \"--partition=milano\", \"--qos=preemptable\"],\n",
    "}\n",
    "\n",
    "cluster = SLURMCluster(**cluster_kwargs)\n",
    "print(cluster.job_script())\n",
    "\n",
    "\n",
    "slurm_jobs = 10\n",
    "cluster.scale(jobs=slurm_jobs)\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your computation calling function\n",
    "\n",
    "This function does a simple operation: add all numbers of a list/array together, but it also sleeps for a random amount of time to simulate real work. In real use cases, this could call another python module, or even run an executable using subprocess module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def costly_simulation(list_param):\n",
    "    time.sleep(random.random())\n",
    "    return sum(list_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try it locally below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.44 ms, sys: 1.77 ms, total: 8.21 ms\n",
      "Wall time: 694 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time costly_simulation([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the set of input parameters to call the function\n",
    "\n",
    "We will generate a set of inputs on which we want to run our simulation function. Here we use Pandas dataframe, but we could also use a simple list. Lets say that our simulation is run with four parameters called param_[a-d]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_a</th>\n",
       "      <th>param_b</th>\n",
       "      <th>param_c</th>\n",
       "      <th>param_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678732</td>\n",
       "      <td>0.204816</td>\n",
       "      <td>0.196032</td>\n",
       "      <td>0.597676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.919661</td>\n",
       "      <td>0.660841</td>\n",
       "      <td>0.094140</td>\n",
       "      <td>0.525106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245555</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.669338</td>\n",
       "      <td>0.719492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.169132</td>\n",
       "      <td>0.461421</td>\n",
       "      <td>0.317992</td>\n",
       "      <td>0.390375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267316</td>\n",
       "      <td>0.422875</td>\n",
       "      <td>0.349523</td>\n",
       "      <td>0.240179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_a   param_b   param_c   param_d\n",
       "0  0.678732  0.204816  0.196032  0.597676\n",
       "1  0.919661  0.660841  0.094140  0.525106\n",
       "2  0.245555  0.104421  0.669338  0.719492\n",
       "3  0.169132  0.461421  0.317992  0.390375\n",
       "4  0.267316  0.422875  0.349523  0.240179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_params = pd.DataFrame(np.random.random(size=(500, 4)),\n",
    "                            columns=['param_a', 'param_b', 'param_c', 'param_d'])\n",
    "input_params.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using Dask, we could call our simulation on all of these parameters using normal Python for loops.\n",
    "\n",
    "Let's only do this on a sample of our parameters as it would be quite long otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 ms, sys: 16.6 ms, total: 58.3 ms\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for parameters in input_params.values[:10]:\n",
    "    result = costly_simulation(parameters)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(1.677255894959546),\n",
       " np.float64(2.19974863963096),\n",
       " np.float64(1.7388058741930705),\n",
       " np.float64(1.3389198640065487),\n",
       " np.float64(1.2798934825044863),\n",
       " np.float64(2.3618483085414885),\n",
       " np.float64(2.020427449580662),\n",
       " np.float64(2.0022995570101996),\n",
       " np.float64(1.5888743147087316),\n",
       " np.float64(1.781019512402362)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not very clever as we can easily parallelize code. \n",
    "\n",
    "There are many ways to parallelize this function in Python with libraries like `multiprocessing`, `concurrent.futures`, `joblib` or others.  These are good first steps.  Dask is a good second step, especially when you want to scale across many machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use [Dask Delayed](http://dask.pydata.org/en/latest/delayed.html) to make our function lazy\n",
    "\n",
    "We can call `dask.delayed` on our funtion to make it lazy.  Rather than compute its result immediately, it records what we want to compute as a task into a graph that we'll run later on parallel hardware. Using `dask.delayed` is a relatively straightforward way to parallelize an existing code base, even if the computation isn't embarrassingly parallel like this one. \n",
    "\n",
    "Calling these lazy functions is now almost free.  In the cell below we only construct a simple graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "lazy_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 ms, sys: 23 μs, total: 1.21 ms\n",
      "Wall time: 979 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for parameters in input_params.values[:10]:\n",
    "    lazy_result = dask.delayed(costly_simulation)(parameters)\n",
    "    lazy_results.append(lazy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('costly_simulation-1cbe1c92-af53-4de2-aa4c-f6f3b0bae049')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in parallel\n",
    "\n",
    "The `lazy_results` list contains information about ten calls to `costly_simulation` that have not yet been run.  Call `.compute()` when you want your result as normal Python objects.\n",
    "\n",
    "If you started `Client()` above then you may want to watch the status page during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dask.compute(*lazy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this was faster than running these same computations sequentially with a for loop.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this on all of our input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "lazy_results = []\n",
    "\n",
    "for parameters in input_params.values:\n",
    "    lazy_result = dask.delayed(costly_simulation)(parameters)\n",
    "    lazy_results.append(lazy_result)\n",
    "    \n",
    "futures = dask.persist(*lazy_results)  # trigger computation in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this go faster, we can add additional workers.\n",
    "\n",
    "(although we're still only working on our local machine, this is more practical when using an actual cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cluster.scale(100)  # ask for a hundred 4-thread workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the Dask dashboard we can see that Dask spreads this work around our cluster, managing load balancing, dependencies, etc..\n",
    "\n",
    "Then get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "results = dask.compute(*futures)\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the [Futures API](http://dask.pydata.org/en/latest/futures.html)\n",
    "\n",
    "The same example can be implemented using Dask's Futures API by using the `client` object itself.  For our use case of applying a function across many inputs both Dask delayed and Dask Futures are equally useful.  The Futures API is a little bit different because it starts work immediately rather than being completely lazy.\n",
    "\n",
    "For example, notice that work starts immediately in the cell below as we submit work to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "for parameters in input_params.values:\n",
    "    future = client.submit(costly_simulation, parameters)\n",
    "    futures.append(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explicitly wait until this work is done and gather the results to our local process by calling `client.gather`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "results = client.gather(futures)\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the code above can be run in fewer lines with `client.map()` function, allowing to call a given function on a list of parameters.\n",
    "\n",
    "As for delayed, we can only start the computation and not wait for results by not calling `client.gather()` right now.\n",
    "\n",
    "It shall be noted that as Dask cluster has already performed tasks launching `costly_simulation` with Futures API on the given input parameters, the call to `client.map()` won't actually trigger any computation, and just retrieve already computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(costly_simulation, input_params.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then just get the results later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to watch the [dashboard's status page](http://127.0.0.1:8787) to watch on going computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some analysis on the results\n",
    "\n",
    "One of the interests of Dask here, outside from API simplicity, is that you are able to gather the result for all your simulations in one call.  There is no need to implement a complex mechanism or to write individual results in a shared file system or object store.\n",
    "\n",
    "Just get your result, and do some computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will just get the results and expand our initial dataframe to have a nice view of parameters vs results for our computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = input_params.copy()\n",
    "output['result'] = pd.Series(results, index=output.index)\n",
    "output.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do some nice statistical plots or save result locally with pandas interface here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "output['result'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['result'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output = output[output['result'] > 2]\n",
    "print(len(filtered_output))\n",
    "filtered_output.to_csv('/tmp/simulation_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling very large simulation with [Bags](http://dask.pydata.org/en/latest/bag.html)\n",
    "\n",
    "The methods above work well for a size of input parameters up to about 100,000.  Above that, the Dask scheduler has trouble handling the amount of tasks to schedule to workers.  The solution to this problem is to bundle many parameters into a single task.\n",
    "You could do this either by making a new function that operated on a batch of parameters and using the delayed or futures APIs on that function.  You could also use the Dask Bag API.  This is described more in the documentation about [avoiding too many tasks](http://dask.pydata.org/en/latest/delayed-best-practices.html#avoid-too-many-tasks).\n",
    "\n",
    "Dask Bags hold onto large sequences in a few partitions.  We can convert our `input_params` sequence into a `dask.bag` collection, asking for fewer partitions (so at most 100,000, which is already huge), and apply our function on every item of the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "b = db.from_sequence(list(input_params.values), npartitions=100)\n",
    "b = b.map(costly_simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time results_bag = b.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking on Dashboard here, you should see only 100 tasks to run instead of 500, each taking 5x more time in average, because each one is actually calling our function 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(results) == np.all(results_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
